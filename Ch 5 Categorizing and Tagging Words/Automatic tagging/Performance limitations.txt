Method 1: consider the number of ambiguous cases a trigram tagger encounters. We can determine the answer empirically using:

cfd = nltk.ConditionalFreqDist(
         ((x[1], y[1], z[0]), z[1])
         for sent in tagged_sents
         for x,y,z in nltk.trigrams(sent))
ambiguous_contexts = [c for c in cfd.conditions() if len(cfd[c]) >  1]
out_of_total = sum(cfd[c].N() for c in ambiguous_contexts) / cfd.N()

Now we know 5% of the cases can be assigned more than one way. If in such cases we always assign a word its most 
likely tag, we can derive a lower bound on the performance of the tagger.
_____________________________________________________________________________________________________________________


Method 2: we can compute what's called the Confusion Matrix, which charts expected tags (from a gold standard) 
against actual tags generated by a tagger:

nltk.ConfusionMatrix(gold_tags, test_tags):

eg:
---

train_tags = [tag for sent in brown.sents(categories = 'editorial') for (word,tag) in tagger.tag(sent)]
gold_tags = [tag for (word,tag) in brown.tagged_words(categories = 'editorial')]
cm = nltk.ConfusionMatrix(gold_tags, train_tags)

print(cm.pretty_format(sort_by_count=True, show_percents=True, truncate=9))

*Look at picture example. The diagonal entries match because these are correctly tagged.
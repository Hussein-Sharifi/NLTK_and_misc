sents = nltk.corpus.treebank_raw.sents()
tokens = []
boundaries = set()
offset = 0

for sent in sents:
    tokens.extend(sent)
    offset += len(sent)
    boundaries.add(offset - 1)

def punct_features(i, tokens):
    return {'next_word_capitalized': tokens[i+1][0].isupper(),
            'prev_word': tokens[i-1],
            'punct': tokens[i],
            'prev_word_is_one-char': len(tokens[i-1]) == 1}

featuresets = [ (punct_features(i, tokens), i in boundaries)
               for i in range(1, len(tokens)-1)
               if tokens[i] in '.!?']

ninety = int( len(featuresets)*.9 )
train_set, test_set = featuresets[:ninety], featuresets[ninety:]

classifier = nltk.NaiveBayesClassifier.train(train_set)
nltk.classify.accuracy(classifier, test_set)


#Now that we can predict word boundaries, let's use this to segment our text

def segment_sentences(words):
    start = 0
    sents = []
    for i,w in enumerate(words):
        if w in '.?!' and classifier.classify(punct_features(i, words)) == True:
            sents.append[start: i+1]
            start = i+1
    if start < len(words)
        sents.append(words[start:])
    return sents